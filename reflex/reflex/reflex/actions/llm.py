# reflex/actions/llm.py
from typing import Dict, Any, Callable
import os
from anthropic import AsyncAnthropic
from .base import ActionBase


@ActionBase.register('llm')
class LLMAction(ActionBase):
    """LLM ê¸°ë°˜ Action (Tool Calling)"""
    
    description = "Use LLM to analyze situation and call tools"
    schema = {
        "system_prompt": {
            "type": "text",
            "description": "System prompt for the LLM",
            "default": "You are a helpful assistant."
        },
        "user_prompt": {
            "type": "text",
            "description": "User prompt/instruction",
            "default": "Execute the task."
        }
    }
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        self.api = config.get('api', 'claude')
        self.model = config.get('model', 'claude-sonnet-4-20250514')
        self.messages = config.get('messages', [])
        self.temperature = config.get('temperature', 0.7)
        
        print(f"[DEBUG] LLMAction initialized:")
        print(f"  - config keys: {list(config.keys())}")
        print(f"  - messages: {self.messages}")
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.api == 'claude':
            api_key = os.environ.get('ANTHROPIC_API_KEY')
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY environment variable not set")
            self.client = AsyncAnthropic(api_key=api_key)
        else:
            raise ValueError(f"Unsupported API: {self.api}")
        
        # íˆ´ ì´ë¦„ ë§¤í•‘ ì´ˆê¸°í™”
        self._tool_name_mapping = {}  # {ìˆœìˆ˜ì´ë¦„: ì „ì²´ì´ë¦„}
    
    async def execute(
        self, 
        event: Dict[str, Any], 
        state: Dict[str, Any],
        tools: Dict[str, Callable],
        trigger: Dict[str, Any]
    ) -> Dict[str, Any]:
        """LLMì—ê²Œ íŒë‹¨ ë§¡ê¸°ê³  Tool Callingìœ¼ë¡œ ì‹¤í–‰"""
        try:
            # 1. Tool ìŠ¤íŽ™ ì¤€ë¹„
            tool_specs = self._prepare_tool_specs(tools)
            
            # 2. ë©”ì‹œì§€ ì²˜ë¦¬ (í…œí”Œë¦¿ ì¹˜í™˜ ì ìš©)
            system_content = None
            user_messages = []
            
            for msg in self.messages:
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                
                # í…œí”Œë¦¿ ì¹˜í™˜ ì ìš©
                resolved_content = self._resolve_template(content, event, state, trigger)
                
                if role == 'system':
                    system_content = resolved_content
                elif role == 'user':
                    user_messages.append({
                        'role': 'user',
                        'content': resolved_content
                    })
                elif role == 'assistant':
                    user_messages.append({
                        'role': 'assistant',
                        'content': resolved_content
                    })
            
            # user ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
            if not user_messages:
                user_messages = [{
                    'role': 'user',
                    'content': 'Please use the available tools as needed.'
                }]
            
            print(f"\nðŸ¤– Calling LLM...")
            print(f"   Model: {self.model}")
            print(f"   Trigger context: {trigger}")
            print(f"   Tools available: {len(tools)}")
            for pure_name, full_name in self._tool_name_mapping.items():
                print(f"      â€¢ {pure_name} (internal: {full_name})")
            
            # system íŒŒë¼ë¯¸í„° ì¤€ë¹„
            if system_content:
                system_param = [{"type": "text", "text": system_content}]
                print(f"   [DEBUG] System param: {system_param}")
            else:
                print(f"   [DEBUG] No system prompt - will omit parameter")
            
            print(f"   [DEBUG] User messages: {user_messages}")
            print(f"   [DEBUG] About to call API...")
            
            # 3. LLM í˜¸ì¶œ
            call_params = {
                "model": self.model,
                "max_tokens": 4096,
                "temperature": self.temperature,
                "messages": user_messages,
            }
            
            # systemì´ ìžˆì„ ë•Œë§Œ ì¶”ê°€
            if system_content:
                call_params["system"] = system_param
            
            # toolsê°€ ìžˆì„ ë•Œë§Œ ì¶”ê°€
            if tool_specs:
                call_params["tools"] = tool_specs
            
            print(f"   [DEBUG] Call params keys: {list(call_params.keys())}")
            
            response = await self.client.messages.create(**call_params)
            
            print(f"   [DEBUG] API call succeeded!")
            
            # 4. Tool Calling ì²˜ë¦¬
            tool_results = []
            text_response = ""
            
            for block in response.content:
                if block.type == 'tool_use':
                    pure_tool_name = block.name  # LLMì´ í˜¸ì¶œí•œ ì´ë¦„ (ì˜ˆ: 'add')
                    tool_args = block.input
                    
                    # ìˆœìˆ˜ ì´ë¦„ -> ì „ì²´ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                    full_tool_name = self._tool_name_mapping.get(pure_tool_name, pure_tool_name)
                    
                    print(f"   ðŸ”§ LLM calls: {pure_tool_name} -> {full_tool_name}({tool_args})")
                    
                    # ì „ì²´ ì´ë¦„ìœ¼ë¡œ íˆ´ ì°¾ê¸°
                    if full_tool_name in tools:
                        try:
                            result = await tools[full_tool_name](**tool_args)
                            tool_results.append({
                                'tool': full_tool_name,
                                'args': tool_args,
                                'result': result
                            })
                            print(f"      âœ“ Result: {result}")
                        except Exception as e:
                            print(f"      âœ— Error: {e}")
                            tool_results.append({
                                'tool': full_tool_name,
                                'args': tool_args,
                                'error': str(e)
                            })
                    else:
                        print(f"      âš ï¸ Tool not available: {full_tool_name}")
                
                elif block.type == 'text':
                    text_response = block.text
                    print(f"   ðŸ’¬ LLM: {text_response}")
            
            return {
                'success': True,
                'tool_calls': tool_results,
                'text': text_response,
                'raw_response': response
            }
            
        except Exception as e:
            print(f"   âŒ LLM execution error: {e}")
            print(f"   [DEBUG] Error type: {type(e)}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e)
            }
    
    def _prepare_tool_specs(self, tools: Dict[str, Callable]) -> list:
        """
        Toolì„ Anthropic API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        'calculator.add' -> 'add'ë¡œ ë³€í™˜í•˜ì—¬ LLMì— ì „ë‹¬
        """
        specs = []
        self._tool_name_mapping.clear()  # ë§¤í•‘ ì´ˆê¸°í™”
        
        for full_tool_name, tool_func in tools.items():
            # ìˆœìˆ˜ íˆ´ ì´ë¦„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì  ì´í›„)
            pure_tool_name = full_tool_name.split('.')[-1] if '.' in full_tool_name else full_tool_name
            
            # ë§¤í•‘ ì €ìž¥
            self._tool_name_mapping[pure_tool_name] = full_tool_name
            
            # í•¨ìˆ˜ì— ë¶™ì–´ìžˆëŠ” MCP ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
            mcp_schema = getattr(tool_func, '_mcp_schema', None)
            
            if mcp_schema:
                # MCP ìŠ¤í‚¤ë§ˆê°€ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë‹¨, ì´ë¦„ì€ ìˆœìˆ˜ ì´ë¦„)
                description = mcp_schema.get('description', f"Execute {pure_tool_name}")
                parameters = mcp_schema.get('parameters', {})
                
                spec = {
                    "name": pure_tool_name,
                    "description": description,
                    "input_schema": parameters
                }
            else:
                # ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìŠ¤íŽ™
                doc = getattr(tool_func, '__doc__', f"Execute {pure_tool_name}")
                
                spec = {
                    "name": pure_tool_name,
                    "description": doc.strip() if doc else f"Execute {pure_tool_name}",
                    "input_schema": {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                }
            
            specs.append(spec)
        
        return specs
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'type': 'llm',
            'api': self.api,
            'model': self.model,
            'messages': self.messages,
            'temperature': self.temperature
        }
    
    def __repr__(self):
        return f"LLMAction(api='{self.api}', model='{self.model}', {len(self.messages)} messages)"